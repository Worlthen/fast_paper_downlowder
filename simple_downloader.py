"""
å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ä¸‹è½½å™¨ - ç®€åŒ–ç‰ˆæœ¬
ä¸ä¾èµ–å¤–éƒ¨åº“çš„åŸºç¡€å®ç°
"""

import sys
import os
import re
import json
import urllib.request
import urllib.parse
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import argparse
import time
import random


class PaperInfo:
    """è®ºæ–‡ä¿¡æ¯ç±»"""
    def __init__(self, title: str, authors: List[str] = None, year: int = None):
        self.title = title
        self.authors = authors or []
        self.year = year
    
    def get_formatted_authors(self) -> str:
        """æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨"""
        if not self.authors:
            return "Unknown"
        
        if len(self.authors) == 1:
            return self.authors[0]
        elif len(self.authors) == 2:
            return " & ".join(self.authors)
        else:
            return f"{self.authors[0]} et al."
    
    def get_search_query(self) -> str:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²"""
        if self.title:
            return self.title
        else:
            query = " ".join(self.authors)
            if self.year:
                query += f" {self.year}"
            return query


class PaperListParser:
    """è®ºæ–‡åˆ—è¡¨è§£æå™¨"""
    
    def __init__(self):
        # ä¿®å¤åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            'standard': re.compile(r'^(?P<authors>[^.]+?)\s*\((?P<year>\d{4})\)\s*(?P<title>[^.]+?)(?:\.(?P<journal>[^.]*))?\.?$'),
            'simple': re.compile(r'^(?P<authors>[^.]+?)\s*\((?P<year>\d{4})\)\s*(?P<title>[^.]+?)(?:\.(?P<journal>[^.]*))?\.?$'),
            'title_only': re.compile(r'^(?P<title>[^.\n]+?)\.?$'),
        }
    
    def parse_line(self, line: str) -> Optional[PaperInfo]:
        """è§£æå•è¡Œè®ºæ–‡ä¿¡æ¯"""
        line = line.strip()
        if not line:
            return None
        
        # å°è¯•ä¸åŒçš„è§£ææ¨¡å¼
        for pattern_name, pattern in self.patterns.items():
            match = pattern.match(line)
            if match:
                if pattern_name == 'title_only':
                    title = match.group('title').strip().rstrip('.')
                    return PaperInfo(title=title)
                else:
                    authors_str = match.group('authors').strip().rstrip('.')
                    year = int(match.group('year')) if match.group('year') else None
                    title = match.group('title').strip().rstrip('.')
                    
                    authors = self._parse_authors(authors_str)
                    return PaperInfo(title=title, authors=authors, year=year)
        
        # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥ï¼Œå°è¯•æå–æ ‡é¢˜
        clean_line = re.sub(r'\(\d{4}\)', '', line).strip()
        if clean_line and len(clean_line) > 10:
            return PaperInfo(title=clean_line)
        
        return None
    
    def _parse_authors(self, authors_str: str) -> List[str]:
        """è§£æä½œè€…å­—ç¬¦ä¸²"""
        authors = []
        authors_str = authors_str.strip().rstrip('.')
        
        # å¤„ç† "Author et al." æ ¼å¼
        if 'et al.' in authors_str.lower():
            main_author = re.sub(r'\s+et\s+al\.?', '', authors_str, flags=re.IGNORECASE).strip()
            if main_author:
                authors.append(main_author)
                authors.append("et al.")
        else:
            # æŒ‰é€—å·åˆ†å‰²
            author_parts = [part.strip() for part in authors_str.split(',')]
            for part in author_parts:
                if part and len(part) > 1:
                    authors.append(part)
        
        return authors
    
    def parse_file(self, file_path: str) -> List[PaperInfo]:
        """è§£æè®ºæ–‡åˆ—è¡¨æ–‡ä»¶"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        
        papers = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('//'):
                    continue
                
                paper = self.parse_line(line)
                if paper:
                    papers.append(paper)
                else:
                    print(f"è­¦å‘Š: ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {line}")
            
            print(f"æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []


class SimplePDFDownloader:
    """ç®€åŒ–ç‰ˆPDFä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str = "./downloads"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / 'pdfs').mkdir(exist_ok=True)
        (self.output_dir / 'metadata').mkdir(exist_ok=True)
    
    def generate_filename(self, paper: PaperInfo, platform: str = 'unknown') -> str:
        """ç”Ÿæˆæ–‡ä»¶å"""
        authors = paper.get_formatted_authors().replace(' ', '_').replace(',', '')
        year = str(paper.year) if paper.year else 'unknown'
        
        # æ¸…ç†æ ‡é¢˜
        title = paper.title[:80]  # é™åˆ¶é•¿åº¦
        title = re.sub(r'[^\w\s-]', '', title)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        title = title.replace(' ', '_')
        
        filename = f"{authors}_{year}_{title}_{platform}.pdf"
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        
        return filename
    
    def download_pdf(self, paper: PaperInfo, pdf_url: str, platform: str = 'unknown') -> bool:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            print(f"ä¸‹è½½PDF: {paper.title}")
            
            # ç”Ÿæˆæ–‡ä»¶åå’Œè·¯å¾„
            filename = self.generate_filename(paper, platform)
            output_path = self.output_dir / 'pdfs' / filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if output_path.exists():
                print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            request = urllib.request.Request(pdf_url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=30) as response:
                if response.status != 200:
                    print(f"ä¸‹è½½å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status}")
                    return False
                
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                
                # ä¿å­˜æ–‡ä»¶
                with open(output_path, 'wb') as f:
                    f.write(response.read())
            
            # éªŒè¯æ–‡ä»¶
            if output_path.stat().st_size < 1024:  # å°äº1KBå¯èƒ½æ— æ•ˆ
                print(f"ä¸‹è½½çš„æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ")
                output_path.unlink()  # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                return False
            
            print(f"ä¸‹è½½æˆåŠŸ: {filename}")
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata(paper, platform, str(output_path))
            
            return True
            
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _save_metadata(self, paper: PaperInfo, platform: str, file_path: str):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            metadata = {
                'title': paper.title,
                'authors': paper.authors,
                'year': paper.year,
                'platform': platform,
                'download_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'file_path': file_path
            }
            
            # ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶å
            base_name = Path(file_path).stem
            metadata_file = self.output_dir / 'metadata' / f"{base_name}.json"
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")


class SimpleGoogleScholarSearcher:
    """ç®€åŒ–ç‰ˆGoogle Scholaræœç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "https://scholar.google.com"
        self.search_path = "/scholar"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def search(self, query: str, max_results: int = 5) -> List[Dict]:
        """æœç´¢å­¦æœ¯è®ºæ–‡"""
        try:
            print(f"Google Scholaræœç´¢: '{query}'")
            
            # æ„å»ºæœç´¢URL
            params = {
                'q': query,
                'num': max_results,
                'hl': 'en'
            }
            
            query_string = urllib.parse.urlencode(params)
            url = f"{self.base_url}{self.search_path}?{query_string}"
            
            print(f"è¯·æ±‚URL: {url}")
            
            # å‘é€è¯·æ±‚
            request = urllib.request.Request(url, headers=self.headers)
            
            with urllib.request.urlopen(request, timeout=30) as response:
                if response.status != 200:
                    print(f"æœç´¢å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status}")
                    return []
                
                html = response.read().decode('utf-8')
            
            # ç®€å•çš„HTMLè§£æ
            results = self._parse_results(html)
            
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_results(self, html: str) -> List[Dict]:
        """è§£ææœç´¢ç»“æœ"""
        results = []
        
        try:
            # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£æ
            # æŸ¥æ‰¾æ ‡é¢˜
            title_pattern = r'<h3[^>]*class="gs_rt"[^>]*>(.*?)<\/h3>'
            titles = re.findall(title_pattern, html, re.DOTALL)
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_pattern = r'<div[^>]*class="gs_ggs"[^>]*>.*?<a[^>]*href="([^"]*)"[^>]*>'
            pdf_links = re.findall(pdf_pattern, html, re.DOTALL)
            
            # æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
            author_pattern = r'<div[^>]*class="gs_a"[^>]*>(.*?)<\/div>'
            authors = re.findall(author_pattern, html, re.DOTALL)
            
            # ç»„åˆç»“æœ
            for i, title in enumerate(titles[:5]):  # é™åˆ¶ç»“æœæ•°é‡
                # æ¸…ç†HTMLæ ‡ç­¾
                title_clean = re.sub(r'<[^>]*>', '', title).strip()
                
                pdf_url = pdf_links[i] if i < len(pdf_links) else None
                author_info = authors[i] if i < len(authors) else ""
                
                # æå–å¹´ä»½
                year_match = re.search(r'\b(19|20)\d{2}\b', author_info)
                year = int(year_match.group()) if year_match else None
                
                # æå–ä½œè€…
                author_clean = re.sub(r'<[^>]*>', '', author_info).strip()
                
                result = {
                    'title': title_clean,
                    'authors': author_clean,
                    'year': year,
                    'pdf_url': pdf_url
                }
                
                results.append(result)
            
        except Exception as e:
            print(f"è§£æç»“æœå¤±è´¥: {e}")
        
        return results


class PaperDownloaderSimple:
    """ç®€åŒ–ç‰ˆè®ºæ–‡ä¸‹è½½å™¨ä¸»ç±»"""
    
    def __init__(self, output_dir: str = "./downloads"):
        self.parser = PaperListParser()
        self.downloader = SimplePDFDownloader(output_dir)
        self.searcher = SimpleGoogleScholarSearcher()
        self.stats = {
            'total_papers': 0,
            'successful_searches': 0,
            'successful_downloads': 0,
            'failed_searches': 0,
            'failed_downloads': 0
        }
    
    def process_paper_list(self, input_file: str, max_results: int = 3) -> Dict:
        """å¤„ç†è®ºæ–‡åˆ—è¡¨"""
        print(f"\nå¼€å§‹å¤„ç†è®ºæ–‡åˆ—è¡¨: {input_file}")
        
        # è§£æè®ºæ–‡åˆ—è¡¨
        papers = self.parser.parse_file(input_file)
        if not papers:
            return {'success': False, 'error': 'No papers found'}
        
        self.stats['total_papers'] = len(papers)
        print(f"è§£æåˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        for i, paper in enumerate(papers, 1):
            print(f"\n[{i}/{len(papers)}] å¤„ç†è®ºæ–‡: {paper.title}")
            
            # æœç´¢è®ºæ–‡
            search_results = self.searcher.search(paper.get_search_query(), max_results)
            
            if search_results:
                self.stats['successful_searches'] += 1
                
                # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªæœ‰PDFé“¾æ¥çš„ç»“æœ
                for result in search_results:
                    if result.get('pdf_url'):
                        success = self.downloader.download_pdf(paper, result['pdf_url'], 'google_scholar')
                        if success:
                            self.stats['successful_downloads'] += 1
                            break
                        else:
                            self.stats['failed_downloads'] += 1
                
                if not any(result.get('pdf_url') for result in search_results):
                    print("æœªæ‰¾åˆ°PDFé“¾æ¥")
                    self.stats['failed_downloads'] += 1
            else:
                print("æœç´¢å¤±è´¥")
                self.stats['failed_searches'] += 1
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«æ£€æµ‹
            time.sleep(random.uniform(1, 3))
        
        return self._generate_report()
    
    def _generate_report(self) -> Dict:
        """ç”ŸæˆæŠ¥å‘Š"""
        report = {
            'total_papers': self.stats['total_papers'],
            'successful_searches': self.stats['successful_searches'],
            'failed_searches': self.stats['failed_searches'],
            'successful_downloads': self.stats['successful_downloads'],
            'failed_downloads': self.stats['failed_downloads'],
            'search_success_rate': self.stats['successful_searches'] / max(self.stats['total_papers'], 1),
            'download_success_rate': self.stats['successful_downloads'] / max(self.stats['successful_searches'], 1)
        }
        
        print("\n" + "=" * 50)
        print("ğŸ“Š ä¸‹è½½ä»»åŠ¡å®Œæˆ")
        print(f"æ€»è®¡è®ºæ–‡: {report['total_papers']}")
        print(f"æœç´¢æˆåŠŸ: {report['successful_searches']} ({report['search_success_rate']:.1%})")
        print(f"ä¸‹è½½æˆåŠŸ: {report['successful_downloads']} ({report['download_success_rate']:.1%})")
        print("=" * 50)
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ä¸‹è½½å™¨ - ç®€åŒ–ç‰ˆæœ¬')
    parser.add_argument('-i', '--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='./downloads', help='è¾“å‡ºç›®å½•')
    parser.add_argument('-n', '--max-results', type=int, default=3, help='æœ€å¤§æœç´¢ç»“æœæ•°')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰3ç¯‡ï¼‰')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“š å­¦æœ¯è®ºæ–‡è‡ªåŠ¨ä¸‹è½½å™¨ - ç®€åŒ–ç‰ˆæœ¬")
    print("=" * 60)
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not Path(args.input).exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = PaperDownloaderSimple(args.output)
    
    try:
        if args.test:
            print("ğŸ”§ æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰3ç¯‡è®ºæ–‡")
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
            import tempfile
            with open(args.input, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                f.writelines(lines[:3])
                test_file = f.name
            
            try:
                report = downloader.process_paper_list(test_file, args.max_results)
            finally:
                Path(test_file).unlink()
        else:
            report = downloader.process_paper_list(args.input, args.max_results)
        
        print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()